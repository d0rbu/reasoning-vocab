# GRPO Training configuration

# Training hyperparameters
num_train_epochs: 4
learning_rate: 5.0e-6
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
effective_batch_size: 8  # per_device * gradient_accum * num_devices

# GRPO-specific parameters
num_generations: 4  # Number of generations per prompt for GRPO
max_prompt_length: 1024
max_completion_length: 512

# Optimization
optimizer: "adamw_torch"
weight_decay: 0.01
max_grad_norm: 1.0
warmup_ratio: 0.1

# Learning rate schedule
lr_scheduler_type: "cosine"

# Logging and checkpointing
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3

# Mixed precision training
bf16: true  # Use bfloat16 if available
fp16: false

# Gradient checkpointing for memory efficiency
gradient_checkpointing: true

# Other settings
dataloader_num_workers: 4
remove_unused_columns: false

